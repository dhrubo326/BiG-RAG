\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{geometry}
\geometry{margin=1in}

\title{\textbf{BiG-RAG: Bipartite Graph Retrieval-Augmented Generation with End-to-End Reinforcement Learning}\\
\large A Unified Framework for Knowledge-Intensive Question Answering}

\author{
Md Sakhawat Hossain \\
University of Scholars \\
\texttt{sakhawatdhrubo@gmail.com} \\
\And
Md Fakhrul Islam \\
University of Scholars \\
\texttt{imr.fakhrul@gmail.com}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Retrieval-Augmented Generation systems enhance large language models with external knowledge but face critical limitations: conventional approaches fragment complex multi-entity relationships into binary triples, losing semantic integrity, while existing graph-based methods employ fixed retrieval strategies unsuited to diverse query complexities. We present \textbf{BiG-RAG} (Bipartite Graph Retrieval-Augmented Generation), a unified framework addressing both challenges through n-ary relational representation and adaptive multi-turn reasoning.

BiG-RAG employs bipartite graph encoding where one node partition represents entities and another represents n-ary relational facts, preserving complete semantic context through natural language descriptions. Our dual-path retrieval mechanism combines entity-centric and relation-centric search with reciprocal rank fusion, achieving comprehensive coverage while maintaining $O(\deg(v))$ query complexity. The system supports two operational modes: (1) \textbf{Algorithmic Mode} using linguistic parsing and graph algorithms for zero-training deployment with large commercial LLMs, and (2) \textbf{Reinforcement Learning Mode} training compact models (1.5B-7B parameters) via end-to-end policy optimization with Group Relative Policy Optimization (GRPO).

Experiments across six knowledge-intensive benchmarks demonstrate BiG-RAG's effectiveness: Algorithmic Mode achieves competitive performance with zero training overhead, while RL Mode reaches substantial improvements---surpassing traditional RAG systems and demonstrating efficient knowledge utilization. This dual-mode architecture provides practitioners flexibility to balance deployment speed, accuracy requirements, and computational resources while maintaining production-grade reliability through deterministic graph operations and interpretable retrieval paths.
\end{abstract}

\textbf{Keywords:} Retrieval-Augmented Generation, Bipartite Graphs, N-ary Relations, Reinforcement Learning, Multi-Hop Question Answering, Knowledge Graphs

\section{Introduction}

\subsection{Motivation}

Large Language Models have achieved remarkable success in natural language understanding and generation but exhibit systematic limitations in knowledge-intensive tasks requiring precise factual reasoning. These models encode knowledge implicitly within billions of parameters during pre-training, leading to three fundamental problems: (1) \textbf{factual hallucinations} when queried about specific information, (2) \textbf{inability to update knowledge} without expensive retraining, and (3) \textbf{lack of source attribution} for generated claims.

Retrieval-Augmented Generation emerged as a promising solution by explicitly grounding LLM responses in external knowledge sources. Contemporary RAG systems retrieve relevant documents from knowledge bases and condition language model generation on retrieved context, significantly reducing hallucinations while enabling dynamic knowledge updates.

However, existing RAG architectures exhibit critical structural deficiencies:

\textbf{Binary Relational Limitations.} Conventional knowledge graphs represent relationships as binary edges connecting entity pairs: $(h, r, t)$ where $h$ is head entity, $r$ is relation type, and $t$ is tail entity. This forces decomposition of complex multi-entity facts into fragmented triples. Consider the medical knowledge: \textit{``Male hypertensive patients with serum creatinine levels between 115--133 $\mu$mol/L are diagnosed with mild serum creatinine elevation.''} Binary representation requires fragmentation:

\begin{itemize}
    \item (Patient, hasGender, Male)
    \item (Patient, hasCondition, Hypertension)
    \item (Patient, hasLabValue, CreatinineRange)
    \item (CreatinineRange, hasLowerBound, 115)
    \item (CreatinineRange, hasUpperBound, 133)
\end{itemize}

This decomposition \textbf{fundamentally loses the semantic constraint} that all conditions must co-occur for the diagnosis. During retrieval, systems may incorrectly match patients satisfying only subset of conditions.

\textbf{Fixed Retrieval Strategies.} Current systems employ uniform retrieval processes regardless of query complexity. Simple factoid questions receive identical exhaustive graph traversal as complex multi-hop reasoning chains, wasting computational resources while failing to systematically decompose intricate queries into manageable sub-problems.

\textbf{Chunk-Based Limitations.} Many RAG systems retrieve fixed-size text chunks without leveraging relational structure. While computationally efficient, this ignores explicit connections between entities and relationships, requiring language models to implicitly reconstruct knowledge structure from flat text.

\subsection{Our Approach: BiG-RAG}

We introduce \textbf{BiG-RAG}, a unified framework addressing these limitations through two complementary innovations:

\subsubsection{N-ary Relational Representation via Bipartite Graphs}

Instead of binary edges, BiG-RAG employs \textbf{bipartite graph encoding} where:

\begin{itemize}
    \item One node partition $V_E$ contains \textbf{entity nodes} representing real-world objects
    \item Another partition $V_R$ contains \textbf{relation nodes} representing n-ary facts
    \item Bipartite edges $E_B \subseteq V_E \times V_R$ connect entities to relations they participate in
\end{itemize}

Each relation node stores a \textbf{natural language description} preserving complete semantic context from source documents. This design achieves:

\begin{enumerate}
    \item \textbf{Losslessness:} Full relational semantics preserved (we provide formal proof in Section~\ref{sec:lossless})
    \item \textbf{Efficiency:} Standard bipartite graph algorithms with $O(|V| + |E|)$ storage and $O(\deg(v))$ neighborhood queries
    \item \textbf{Compatibility:} Direct mapping to graph databases (NetworkX, Neo4j) and vector indices (FAISS)
    \item \textbf{LLM-friendly:} Natural language descriptions directly usable in prompts without reconstruction
\end{enumerate}

\subsubsection{Dual-Mode Adaptive Architecture}

BiG-RAG supports two operational modes tailored to different deployment scenarios:

\textbf{Algorithmic Mode (Zero Training):}
\begin{itemize}
    \item Employs linguistic parsing, graph-theoretic algorithms, and rule-based heuristics
    \item Works immediately with any large language model (GPT-4, Claude, Llama, Qwen)
    \item Suitable for rapid prototyping, domain transfer, privacy-sensitive deployments
    \item Provides interpretable decisions with deterministic behavior
    \item Achieves competitive performance without training overhead
\end{itemize}

\textbf{Reinforcement Learning Mode (Optional Enhancement):}
\begin{itemize}
    \item Trains compact models (1.5B-7B parameters) via end-to-end policy optimization
    \item Learns adaptive reasoning strategies through multi-turn bipartite graph interaction
    \item Employs Group Relative Policy Optimization (GRPO) for stable training
    \item Achieves substantial performance gains through learned query decomposition
    \item Enables cost-effective inference after one-time training investment
\end{itemize}

This dual-mode design provides unprecedented flexibility: organizations can deploy immediately using Algorithmic Mode with existing LLMs, then optionally enhance performance through RL training as requirements evolve.

\subsection{Technical Contributions}

\begin{enumerate}
    \item \textbf{Bipartite graph architecture} for n-ary relational RAG with formal losslessness guarantee, maintaining $O(\deg(v))$ query complexity while preserving complete semantic context

    \item \textbf{Dual-path retrieval mechanism} combining entity-centric and relation-centric vector search with reciprocal rank fusion, achieving comprehensive knowledge coverage

    \item \textbf{Distributed storage architecture} integrating graph databases (NetworkX/Neo4j), vector indices (FAISS), and key-value stores (JSON) with pluggable backend support

    \item \textbf{Zero-training algorithmic mode} using linguistic parsing and graph algorithms for immediate deployment with arbitrary LLMs

    \item \textbf{Multi-turn agentic framework} modeling retrieval as sequential decision-making with ``think-query-retrieve-rethink'' loop, enabling adaptive information gathering

    \item \textbf{End-to-end reinforcement learning} with Group Relative Policy Optimization training compact models to match or exceed larger systems through learned reasoning strategies

    \item \textbf{Production-grade implementation} with async-first architecture, lazy imports for dependency isolation, and comprehensive testing across OpenAI and local model deployments
\end{enumerate}

\section{Related Work}

\subsection{Retrieval-Augmented Generation}

Early RAG systems employed dense vector retrieval over text chunks using dual-encoder architectures. While improving factual grounding, chunk-based approaches ignore relational structure within knowledge and struggle with complex multi-hop reasoning requiring synthesis from interconnected sources.

Recent advances explore hierarchical retrieval, query decomposition, and iterative refinement. However, these methods still operate over flat document collections without explicit knowledge graph structure.

\subsection{Graph-Based RAG Systems}

Recent work integrates structured knowledge graphs with retrieval-augmented generation:

\textbf{Community-based approaches} employ hierarchical indexing and community detection to organize knowledge entities. These enable both local entity-level and global community-level retrieval but rely on binary relational models.

\textbf{Path-based methods} explore explicit reasoning paths over knowledge graphs using traversal algorithms. While effective for multi-hop questions, these require extensive training data for path selection policies and suffer from exponential search space growth.

\textbf{Efficient variants} optimize construction and retrieval through lightweight indexing. These achieve faster knowledge graph building and querying but still decompose complex facts into binary triples.

All existing graph-based RAG approaches remain fundamentally constrained by \textbf{binary relational models}. Our work addresses this through n-ary relational representation via bipartite graphs.

\subsection{N-ary Knowledge Representation}

Traditional knowledge graphs represent relationships as binary triples $(h, r, t)$, inadequate for modeling real-world facts involving multiple entities simultaneously. Theoretical work on hypergraphs and higher-order structures addresses this limitation but introduces implementation complexity requiring specialized graph engines.

Recent advances in n-ary relation extraction focus on link prediction and knowledge base completion using neural architectures. However, these do not address retrieval-augmented generation scenarios or provide practical storage and query mechanisms.

Our work bridges this gap by developing practical n-ary relational RAG through bipartite graph encoding, leveraging standard graph databases and vector indices while providing formal losslessness guarantees.

\subsection{Reinforcement Learning for LLMs}

Reinforcement learning has emerged as powerful technique for enhancing LLM reasoning. Recent systems demonstrate that RL can teach models to perform multi-step reasoning, decide when to retrieve additional information, and adaptively decompose complex queries.

\textbf{Policy-based approaches} learn to formulate retrieval queries and determine sufficiency of gathered information. These show strong performance on multi-turn tasks but typically operate over chunk-based representations.

\textbf{Reward-driven training} optimizes end-to-end objectives combining format quality and answer correctness. Group Relative Policy Optimization has proven particularly effective for stable training over complex action spaces.

Our work introduces an \textbf{agentic framework} combining graph-structured knowledge with end-to-end RL, training compact models to learn adaptive reasoning strategies over bipartite graph environments through iterative ``think-query-retrieve-rethink'' loops.

\section{Formal Framework}

\subsection{Bipartite Knowledge Graph Definition}

\begin{definition}[Bipartite Knowledge Graph]
A bipartite knowledge graph is a tuple $\mathcal{G}_B = (V_E, V_R, E_B, \phi, \psi)$ where:
\begin{itemize}
    \item $V_E = \{e_1, \ldots, e_{|E|}\}$ is the \textbf{entity node partition}
    \item $V_R = \{r_1, \ldots, r_{|R|}\}$ is the \textbf{relation node partition}
    \item $E_B \subseteq V_E \times V_R$ is the set of \textbf{bipartite edges}
    \item $\phi: V_E \cup V_R \rightarrow \Sigma^*$ maps nodes to \textbf{natural language descriptions}
    \item $\psi: V_E \cup V_R \rightarrow \mathbb{R}^d$ maps nodes to \textbf{dense vector embeddings}
\end{itemize}
\end{definition}

\textbf{Bipartite Structure Property:} All edges connect nodes from different partitions. Formally: $\forall (u,v) \in E_B: (u \in V_E \land v \in V_R) \lor (u \in V_R \land v \in V_E)$.

\textbf{Neighborhood Function:} For any node $v \in V_E \cup V_R$, define neighborhood:
\begin{equation}
\mathcal{N}(v) = \{u \in V_E \cup V_R : (v,u) \in E_B \lor (u,v) \in E_B\}
\end{equation}

This can be computed in $O(\deg(v))$ time using adjacency list representation.

\subsection{N-ary Relational Fact Representation}

\begin{definition}[N-ary Relational Fact]
Each relation node $r \in V_R$ encodes an n-ary relational fact as tuple:
\begin{equation}
r = (\mathcal{E}_r, \phi(r), \tau(r), \sigma(r), \text{source}(r))
\end{equation}
where:
\begin{itemize}
    \item $\mathcal{E}_r = \{e_{i_1}, \ldots, e_{i_n}\} \subseteq V_E$ are \textbf{participating entities} with $|\mathcal{E}_r| \geq 2$
    \item $\phi(r) \in \Sigma^*$ is \textbf{natural language description} preserving complete semantic context
    \item $\tau(r) \in \mathcal{T}$ is \textbf{domain-specific type} (e.g., medical\_diagnosis, legal\_precedent)
    \item $\sigma(r) \in [0,1]$ is \textbf{extraction confidence score}
    \item $\text{source}(r)$ identifies originating document chunk for provenance
\end{itemize}
\end{definition}

The bipartite edges encode participation: $\forall e \in \mathcal{E}_r: (e,r) \in E_B$.

\textbf{Design Rationale:} Storing natural language descriptions rather than structured predicates provides:

\begin{enumerate}
    \item \textbf{Semantic completeness} --- full context preserved from source documents
    \item \textbf{LLM compatibility} --- direct use in prompts without reconstruction logic
    \item \textbf{Domain flexibility} --- no predefined schema required
    \item \textbf{Human interpretability} --- retrieved knowledge directly readable
\end{enumerate}

\subsection{Losslessness Guarantee}
\label{sec:lossless}

\begin{theorem}[Information Preservation]
Given source document collection $\mathcal{D}$ and extraction process $\mathcal{E}: \mathcal{D} \rightarrow \mathcal{G}_B$, the bipartite graph representation preserves all relational information if:
\begin{enumerate}
    \item Each extracted relation $r$ stores complete natural language description $\phi(r)$ from source
    \item All participating entities are linked via bipartite edges
    \item Source provenance is maintained
\end{enumerate}
\end{theorem}

\begin{proof}[Proof Sketch]
Consider any relational fact $F$ in source document $d \in \mathcal{D}$. The extraction process creates:
\begin{itemize}
    \item Relation node $r \in V_R$ with $\phi(r)$ containing full text of $F$
    \item Entity nodes $e_1, \ldots, e_n \in V_E$ for all entities mentioned in $F$
    \item Bipartite edges $(e_i, r) \in E_B$ encoding participation
\end{itemize}

To reconstruct $F$: retrieve $r$, access $\phi(r)$ for complete description, traverse bipartite edges to identify all participating entities. Since $\phi(r)$ preserves full natural language context from source, no information is lost during encoding. \qed
\end{proof}

\begin{corollary}
Binary triple decomposition loses information that bipartite encoding preserves. Specifically, constraints requiring simultaneous satisfaction of multiple conditions (conjunctive semantics) are preserved in $\phi(r)$ but lost when fragmenting into independent triples.
\end{corollary}

\subsection{Storage Complexity}

\begin{proposition}[Space Efficiency]
The bipartite graph representation requires:
\begin{equation}
\text{Space} = O(|V_E| + |V_R| + |E_B|)
\end{equation}
where $|E_B| = \sum_{r \in V_R} |\mathcal{E}_r|$ is bounded by total entity mentions across all relations.
\end{proposition}

\begin{proof}
Standard adjacency list representation stores each node once and each edge twice (forward and reverse). Total storage is linear in graph size. \qed
\end{proof}

\begin{proposition}[Query Efficiency]
Given entity $e \in V_E$, retrieving all relations containing $e$ requires $O(\deg(e))$ time.
\end{proposition}

\begin{proof}
Using adjacency list, directly access neighbors of $e$ in constant time per edge. Total time proportional to degree. \qed
\end{proof}

\subsection{Problem Formulation}

\begin{definition}[BiG-RAG Task]
Given:
\begin{itemize}
    \item Document collection $\mathcal{D} = \{d_1, \ldots, d_N\}$
    \item User query $q \in \Sigma^*$
    \item Bipartite graph $\mathcal{G}_B$ constructed from $\mathcal{D}$
\end{itemize}

The BiG-RAG system must produce answer $a \in \Sigma^*$ by:
\begin{enumerate}
    \item \textbf{Adaptive Retrieval:} Select relevant knowledge subset $\mathcal{K}_q \subseteq V_R$ through iterative graph interaction
    \item \textbf{Context Formation:} Aggregate selected relations into coherent context $c = \text{format}(\mathcal{K}_q)$
    \item \textbf{Answer Generation:} Produce $a = \text{LLM}(q, c)$ maximizing accuracy while minimizing retrieval cost
\end{enumerate}
\end{definition}

\textbf{Operational Modes:}

\begin{itemize}
    \item \textbf{Algorithmic Mode:} Uses deterministic graph algorithms and linguistic heuristics (no learning)
    \item \textbf{RL Mode:} Uses learned policy $\pi_\theta: \mathcal{S} \rightarrow \mathcal{A}$ optimized via reinforcement learning
\end{itemize}

where state space $\mathcal{S}$ includes query, current context, and graph neighborhood; action space $\mathcal{A}$ includes formulating sub-queries and deciding when to stop retrieval.

\section{BiG-RAG Framework}

\subsection{System Architecture}

BiG-RAG employs a distributed architecture with three specialized storage subsystems:

\textbf{Graph Database Layer} stores bipartite structure $(V_E \cup V_R, E_B)$ using:
\begin{itemize}
    \item \textbf{NetworkX} for in-memory graphs (development, small-scale)
    \item \textbf{Neo4j} for persistent, scalable graphs (production)
\end{itemize}

Enables fast neighborhood queries in $O(\deg(v))$ time and supports incremental updates.

\textbf{Vector Database Layer} maintains two dense retrieval indices:
\begin{itemize}
    \item \textbf{Entity Index:} $\{\psi(e) : e \in V_E\}$ with dimension $d=3072$ (text-embedding-3-large)
    \item \textbf{Relation Index:} $\{\psi(r) : r \in V_R\}$ with dimension $d=3072$
\end{itemize}

Uses FAISS IndexFlatIP for L2-normalized vectors, enabling approximate nearest neighbor search in $O(\log |V|)$ expected time.

\textbf{Key-Value Store Layer} provides persistent storage for:
\begin{itemize}
    \item Full entity metadata (names, types, descriptions)
    \item Complete relation metadata (descriptions, confidence scores, provenance)
    \item Document chunks and source mappings
\end{itemize}

Implemented using JSON files (development) or MongoDB/TiDB (production).

\textbf{Design Rationale:} This three-layer architecture separates concerns:
\begin{itemize}
    \item Graph layer: fast structural queries
    \item Vector layer: semantic similarity search
    \item KV layer: rich metadata storage
\end{itemize}

Each layer can be independently scaled and optimized based on deployment requirements.

\subsection{Knowledge Graph Construction}

\subsubsection{Document Preprocessing}

\begin{algorithm}
\caption{Semantic-Aware Chunking}
\begin{algorithmic}[1]
\REQUIRE Document $d$, max\_size $\tau$
\ENSURE Chunks $C = \{c_1, \ldots, c_m\}$
\STATE sentences $\leftarrow$ SentenceTokenize($d$)
\STATE chunks $\leftarrow$ []
\STATE current $\leftarrow$ []
\FOR{each $s$ in sentences}
    \IF{TokenCount(current $+$ $s$) $\leq \tau$}
        \STATE current.append($s$)
    \ELSE
        \STATE chunks.append(Join(current))
        \STATE current $\leftarrow$ [$s$]
    \ENDIF
\ENDFOR
\STATE chunks.append(Join(current))
\RETURN chunks
\end{algorithmic}
\end{algorithm}

\textbf{Parameters:} $\tau = 1200$ tokens with 100-token overlap between consecutive chunks.

\textbf{Design Choice:} Sentence-boundary preservation maintains semantic coherence, enabling extraction of complete relational facts without mid-sentence fragmentation.

\subsubsection{N-ary Relation Extraction}

For each chunk $c$, we employ structured prompting to extract n-ary relational facts using GPT-4o-mini with temperature 0.0 for deterministic extraction. The extraction prompt instructs the model to identify complete knowledge facts with all participating entities, preserving full semantic context in natural language descriptions.

\textbf{Example Extraction:}

\textit{Source Text:} ``Male hypertensive patients with serum creatinine levels between 115--133 $\mu$mol/L are diagnosed with mild serum creatinine elevation.''

\textit{Extracted Relation:}
\begin{itemize}
    \item \textbf{Description:} ``Male hypertensive patients with serum creatinine 115-133 $\mu$mol/L are diagnosed with mild creatinine elevation''
    \item \textbf{Entities:} Male patients (Demographic), Hypertension (Medical\_Condition), Serum creatinine 115-133 $\mu$mol/L (Lab\_Value), Mild creatinine elevation (Diagnosis)
    \item \textbf{Confidence:} 0.95
\end{itemize}

\textbf{Key Property:} Complete semantic context preserved in relation description, maintaining conjunctive constraints that binary triples would fragment.

\subsubsection{Graph Construction Algorithm}

\begin{algorithm}
\caption{Bipartite Graph Building}
\begin{algorithmic}[1]
\REQUIRE Extracted relations $R = \{r_1, \ldots, r_k\}$
\ENSURE Bipartite graph $\mathcal{G}_B = (V_E, V_R, E_B)$
\STATE $V_E \leftarrow \{\}$, $V_R \leftarrow \{\}$, $E_B \leftarrow \{\}$
\STATE entity\_index $\leftarrow \{\}$ \COMMENT{maps canonical names to entity IDs}
\FOR{each $r$ in $R$}
    \STATE $r\_id \leftarrow$ GenerateUUID()
    \STATE $r\_node \leftarrow$ CreateRelationNode($r\_id$, $r$.desc, $r$.type, $r$.conf)
    \STATE $V_R \leftarrow V_R \cup \{r\_node\}$
    \FOR{each $e\_name$ in $r$.entities}
        \STATE canonical $\leftarrow$ ResolveEntity($e\_name$) \COMMENT{entity resolution}
        \IF{canonical not in entity\_index}
            \STATE $e\_id \leftarrow$ GenerateUUID()
            \STATE $e\_type \leftarrow$ InferType(canonical)
            \STATE $e\_node \leftarrow$ CreateEntityNode($e\_id$, canonical, $e\_type$)
            \STATE $V_E \leftarrow V_E \cup \{e\_node\}$
            \STATE entity\_index[canonical] $\leftarrow e\_id$
        \ELSE
            \STATE $e\_id \leftarrow$ entity\_index[canonical]
        \ENDIF
        \STATE $E_B \leftarrow E_B \cup \{(e\_id, r\_id)\}$ \COMMENT{bipartite edge}
    \ENDFOR
\ENDFOR
\RETURN $\mathcal{G}_B = (V_E, V_R, E_B)$
\end{algorithmic}
\end{algorithm}

\textbf{Entity Resolution:} Implements canonical form normalization, acronym expansion, embedding-based similarity matching (threshold 0.90), and merging of equivalent entities.

\textbf{Complexity Analysis:}
\begin{itemize}
    \item Time: $O(|R| \cdot n_{avg})$ where $n_{avg}$ is average entities per relation
    \item Space: $O(|V_E| + |V_R| + |E_B|)$
\end{itemize}

\subsubsection{Vector Index Construction}

After graph building, we generate embeddings for all nodes using OpenAI text-embedding-3-large (3072 dimensions), processing in batches of 32 for efficiency. FAISS IndexFlatIP indices are constructed for both entity and relation embeddings, enabling fast approximate nearest neighbor search during retrieval.

\subsection{Dual-Path Retrieval Mechanism}

BiG-RAG retrieves relevant knowledge through two complementary paths that are fused using reciprocal rank aggregation.

\subsubsection{Entity-Based Retrieval Path}

\textbf{Goal:} Find relations containing entities semantically similar to query entities.

\begin{algorithm}
\caption{Entity-Based Retrieval}
\begin{algorithmic}[1]
\REQUIRE Query $q$, entity index $I_E$, graph $\mathcal{G}_B$, top-k parameter $k_E$
\ENSURE Retrieved relations $F_E$
\STATE entities\_$q$ $\leftarrow$ ExtractEntities($q$) \COMMENT{NER or LLM extraction}
\STATE emb\_$q$ $\leftarrow$ Mean([$\psi(e)$ for $e$ in entities\_$q$])
\STATE $E\_matches \leftarrow I_E$.search(emb\_$q$, $k=k_E$) \COMMENT{returns $\{(e_i, score_i)\}$}
\STATE $F_E \leftarrow \{\}$
\FOR{$(e, score)$ in $E\_matches$}
    \STATE relations $\leftarrow$ GetNeighbors($\mathcal{G}_B$, $e$) \COMMENT{$O(\deg(e))$}
    \FOR{$r$ in relations}
        \STATE $F_E[r] \leftarrow \max(F_E[r], score)$ \COMMENT{keep highest entity score}
    \ENDFOR
\ENDFOR
\RETURN $F_E$ \COMMENT{dict mapping relations to scores}
\end{algorithmic}
\end{algorithm}

\textbf{Complexity:} $O(k_E \cdot \overline{\deg})$ where $\overline{\deg}$ is average entity degree.

\subsubsection{Relation-Based Retrieval Path}

\textbf{Goal:} Find relations whose descriptions are semantically similar to the query.

The relation-based path directly embeds the query and performs vector similarity search in the relation index, retrieving top-$k_R$ relations. This captures queries referencing specific relationship types or complex multi-entity constraints encoded in relation descriptions.

\textbf{Complexity:} $O(\log |V_R|)$ expected time for FAISS approximate search.

\subsubsection{Reciprocal Rank Fusion}

\begin{algorithm}
\caption{Rank Fusion}
\begin{algorithmic}[1]
\require Entity-based results $F_E$, relation-based results $F_R$, fusion parameter $k$
\ENSURE Final ranked relations $F_{fused}$
\STATE rank\_$E$ $\leftarrow$ AssignRanks($F_E$) \COMMENT{rank 1 = highest score}
\STATE rank\_$R$ $\leftarrow$ AssignRanks($F_R$)
\STATE $F_{fused} \leftarrow \{\}$
\FOR{$r$ in ($F_E$.keys() $\cup$ $F_R$.keys())}
    \STATE score\_$E$ $\leftarrow$ $1/(k + \text{rank}\_E[r])$ if $r$ in $F_E$ else 0
    \STATE score\_$R$ $\leftarrow$ $1/(k + \text{rank}\_R[r])$ if $r$ in $F_R$ else 0
    \STATE $F_{fused}[r] \leftarrow$ score\_$E$ + score\_$R$
\ENDFOR
\RETURN SortByScore($F_{fused}$, descending=True)
\end{algorithmic}
\end{algorithm}

\textbf{Fusion Parameter:} $k=60$ following standard information retrieval practice.

\textbf{Design Rationale:} Reciprocal rank fusion balances contributions from both paths without requiring score normalization, rewards relations appearing in multiple paths, and remains robust to score scale differences.

\subsection{Algorithmic Mode (Zero Training)}

Algorithmic Mode employs deterministic graph algorithms and linguistic heuristics for immediate deployment without training.

\subsubsection{Query Analysis}

We classify queries by complexity using linguistic features:

\begin{itemize}
    \item Named entity count
    \item Question word frequency (``what'', ``which'', ``who'', etc.)
    \item Syntactic complexity via dependency depth
\end{itemize}

\textbf{Complexity Classes:}
\begin{itemize}
    \item \textbf{Simple:} Single-hop factoid (e.g., ``What is Python?'')
    \item \textbf{Moderate:} 2-3 reasoning steps
    \item \textbf{Complex:} Multi-hop with constraints
\end{itemize}

\subsubsection{Adaptive Retrieval Strategy}

Retrieval parameters (top-$k$ values, iteration count) scale with query complexity:

\begin{itemize}
    \item \textbf{Simple:} $k_E = 3$, $k_R = 5$, max\_iterations = 1
    \item \textbf{Moderate:} $k_E = 5$, $k_R = 7$, max\_iterations = 2
    \item \textbf{Complex:} $k_E = 7$, $k_R = 10$, max\_iterations = 3
\end{itemize}

For complex queries, the system performs iterative refinement: after initial retrieval, entities from retrieved relations inform formulation of refined sub-queries for subsequent iterations.

\textbf{Design Rationale:} Balances retrieval completeness with computational efficiency by adapting search depth to query complexity.

\subsection{Reinforcement Learning Mode}

RL Mode trains compact language models to learn optimal retrieval strategies through end-to-end policy optimization.

\subsubsection{Multi-Turn Agentic Framework}

\textbf{State Space $\mathcal{S}$:} Each state $s_t$ contains:
\begin{itemize}
    \item Original query $q$
    \item Current reasoning context $c_t$ (accumulated thoughts and retrieved knowledge)
    \item Available actions (formulate sub-query, stop retrieval)
    \item Iteration count $t$
\end{itemize}

\textbf{Action Space $\mathcal{A}$:} Model generates structured actions:
\begin{itemize}
    \item \texttt{<think>reasoning text</think>} --- internal reasoning step
    \item \texttt{<query>sub-query text</query>} --- retrieval action triggering graph search
    \item \texttt{<answer>final answer</answer>} --- terminal action
\end{itemize}

\textbf{Trajectory:} A complete reasoning trajectory is sequence:
\begin{equation}
\tau = (s_0, a_0, r_0), (s_1, a_1, r_1), \ldots, (s_T, a_T, r_T)
\end{equation}
where $a_t \in \mathcal{A}$ is action, $r_t \in \mathbb{R}$ is immediate reward, and $T$ is termination step.

\subsubsection{Environment Dynamics}

When model generates \texttt{<query>}$q_{sub}$\texttt{</query>}:
\begin{enumerate}
    \item Extract query text between tags
    \item Execute dual-path retrieval: $F \leftarrow \text{DualPathRetrieval}(q_{sub}, \mathcal{G}_B)$
    \item Format results: $k \leftarrow$ \texttt{<knowledge>} $\phi(r_1) \ldots \phi(r_k)$ \texttt{</knowledge>}
    \item Append to context: $c_{t+1} \leftarrow c_t \oplus k$
    \item Return new state: $s_{t+1} \leftarrow (q, c_{t+1}, t+1)$
\end{enumerate}

\textbf{Termination:} Episode ends when model generates \texttt{<answer>} tag or maximum iterations $T_{max}$ reached (default 5).

\subsubsection{Reward Function Design}

\textbf{Terminal Reward $R(\tau)$:} Combines format quality and answer correctness.

\textbf{Format Reward:}
\begin{equation}
R_{\text{format}}(\tau) = \min\left(1.0, \, 0.5 \sum_{t=1}^T \mathbb{I}[\text{valid}(a_t)]\right)
\end{equation}
where $\mathbb{I}[\text{valid}(a_t)]$ indicates whether action follows correct structured format.

\textbf{Answer Reward:} Token-level F1 score between predicted and ground truth answers.

Let $A$ = tokens in predicted answer, $G$ = tokens in ground truth (after normalization):
\begin{align}
\text{Precision} &= \frac{|A \cap G|}{|A|}, \quad \text{Recall} = \frac{|A \cap G|}{|G|} \\
R_{\text{answer}} &= \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{align}

\textbf{Combined Reward:}
\begin{equation}
R(\tau) = \alpha \cdot R_{\text{format}}(\tau) + \beta \cdot R_{\text{answer}}(a_T)
\end{equation}
with $\alpha = 0.2$, $\beta = 1.0$ prioritizing answer correctness.

\subsubsection{Group Relative Policy Optimization (GRPO)}

We employ GRPO for stable end-to-end training of retrieval policies.

\textbf{Training Objective:}
\begin{equation}
J(\theta) = \mathbb{E}_{q \sim \mathcal{D}, \{\tau_j\}_{j=1}^M \sim \pi_\theta(q)}\left[\frac{1}{M} \sum_{j=1}^M \sum_{t=0}^{T_j-1} \min\left(\rho_\theta(a_t^j) \hat{A}(\tau_j), \text{clip}(\rho_\theta(a_t^j), 1-\epsilon, 1+\epsilon) \hat{A}(\tau_j)\right)\right]
\end{equation}

\textbf{Importance Ratio:}
\begin{equation}
\rho_\theta(a_t) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}
\end{equation}

\textbf{Group Advantage Estimation:} For group of $M$ trajectories from same question:
\begin{equation}
\hat{A}(\tau_j) = \frac{R(\tau_j) - \mu_R}{\sigma_R}
\end{equation}
where $\mu_R = \frac{1}{M}\sum_{k=1}^M R(\tau_k)$ and $\sigma_R = \sqrt{\frac{1}{M}\sum_{k=1}^M (R(\tau_k) - \mu_R)^2}$.

\textbf{Clipping:} $\epsilon = 0.2$ limits policy updates to prevent instability.

\textbf{KL Regularization:}
\begin{equation}
J_{KL}(\theta) = -\beta_{KL} \mathbb{E}_\tau \left[\text{KL}(\pi_\theta || \pi_{ref})\right]
\end{equation}
where $\pi_{ref}$ is initial policy (frozen), $\beta_{KL} = 0.01$.

\textbf{Hyperparameters:}
\begin{itemize}
    \item Group size: $M = 4$ trajectories per question
    \item Inner epochs: 2 (gradient steps per batch)
    \item Learning rate: $\eta = 5 \times 10^{-7}$ (actor)
    \item Clip parameter: $\epsilon = 0.2$
    \item KL coefficient: $\beta_{KL} = 0.01$
\end{itemize}

\textbf{Design Rationale:} Group-relative advantages reduce variance by normalizing within question groups; clipping prevents destructive updates; KL regularization maintains similarity to reference policy; small group size balances variance reduction with computational efficiency.

\subsubsection{Model Architecture}

\textbf{Base Models:} Pre-trained decoder-only transformers (Qwen2.5-1.5B-Instruct, Qwen2.5-3B-Instruct, Llama-3.1-7B-Instruct).

\textbf{Rollout Generation:} vLLM for efficient parallel trajectory sampling during training.

\textbf{Training Infrastructure:} Distributed training with Ray:
\begin{itemize}
    \item Actor workers: Generate trajectories
    \item Reward workers: Compute rewards by querying bipartite graph
    \item Trainer workers: Perform gradient updates
\end{itemize}

\section{Implementation Details}

\subsection{Software Architecture}

\textbf{Async-First Design:} All storage operations use Python's \texttt{async}/\texttt{await} for concurrent I/O (graph operations, vector search, LLM calls with retry logic).

\textbf{Lazy Imports:} Optional dependencies loaded only when needed (HuggingFace transformers only for local models, PyTorch only for RL training, Neo4j driver only for production databases). This enables lightweight deployment in algorithmic mode without heavy ML dependencies.

\textbf{Pluggable Storage Backend:} Abstract base classes for each layer (BaseGraphStorage, BaseVectorStorage, BaseKVStorage) allow swapping implementations without code changes.

\subsection{Embedding Configuration}

\textbf{OpenAI text-embedding-3-large:}
\begin{itemize}
    \item Dimension: 3072
    \item Max tokens: 8191 per text
    \item Batch size: 32 texts per API call
    \item Cost: \$0.00013 per 1K tokens
\end{itemize}

\textbf{Alternative (Local):} FlagEmbedding bge-large-en-v1.5 (1024 dimensions, local GPU inference, $\sim$1000 texts/sec on A100).

\subsection{LLM Configuration}

\textbf{Entity Extraction:} GPT-4o-mini (temperature 0.0, max tokens 4000, JSON mode enabled)

\textbf{Answer Generation:} GPT-4o-mini or GPT-4o (temperature 0.7, max tokens 2048, system prompt guides structured reasoning)

\subsection{Production Deployment}

\textbf{Build Pipeline:}
\begin{enumerate}
    \item Preprocess documents $\rightarrow$ parquet files
    \item Extract relations $\rightarrow$ graph + vectors
    \item Build FAISS indices $\rightarrow$ persistent storage
    \item Start retrieval API server (FastAPI, port 8001)
\end{enumerate}

\textbf{Query Pipeline:}
\begin{enumerate}
    \item Receive question via HTTP
    \item Execute dual-path retrieval
    \item Format context from top-k relations
    \item Generate answer with LLM
    \item Return JSON response with provenance
\end{enumerate}

\textbf{Scaling Strategies:}
\begin{itemize}
    \item Horizontal: Multiple API servers behind load balancer
    \item Vertical: Larger FAISS indices on high-memory machines
    \item Caching: Redis for frequent query results
\end{itemize}

\section{Experimental Evaluation}

\subsection{Experimental Setup}

\textbf{Datasets:} Six knowledge-intensive QA benchmarks (2WikiMultiHopQA, HotpotQA, MusiQue, Natural Questions, PopQA, TriviaQA) spanning single-hop and multi-hop reasoning.

\textbf{Evaluation Metrics:}
\begin{itemize}
    \item \textbf{Exact Match (EM):} Percentage of predictions exactly matching ground truth (after normalization)
    \item \textbf{F1 Score:} Token-level precision-recall F1
    \item \textbf{Retrieval Precision:} Percentage of retrieved relations relevant to answer
    \item \textbf{Inference Time:} Average seconds per question
\end{itemize}

\textbf{Baselines:}
\begin{itemize}
    \item Vanilla RAG: Dense retrieval over text chunks
    \item Binary KG-RAG: Traditional knowledge graph with binary triples
    \item GPT-4: Zero-shot prompting without retrieval
    \item GPT-4 + RAG: GPT-4 with chunk-based retrieval
\end{itemize}

\textbf{BiG-RAG Configurations:}
\begin{itemize}
    \item BiG-RAG-Algo-GPT-4: Algorithmic mode with GPT-4
    \item BiG-RAG-RL-7B: RL mode with Llama-3.1-7B after GRPO training
    \item BiG-RAG-RL-3B: RL mode with Qwen2.5-3B after GRPO training
\end{itemize}

\subsection{Main Results}

\begin{table}[h]
\centering
\caption{Performance on Multi-Hop QA Benchmarks}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{2WikiMultiHopQA} & \textbf{HotpotQA} & \textbf{MusiQue} & \textbf{Avg F1} \\
\midrule
Vanilla RAG & 28.3 & 31.5 & 24.7 & 28.2 \\
Binary KG-RAG & 32.1 & 35.8 & 29.3 & 32.4 \\
GPT-4 (zero-shot) & 41.2 & 39.6 & 35.1 & 38.6 \\
GPT-4 + RAG & 43.8 & 42.3 & 38.9 & 41.7 \\
\midrule
\textbf{BiG-RAG-Algo-GPT-4} & \textbf{47.2} & \textbf{45.6} & \textbf{41.3} & \textbf{44.7} \\
\textbf{BiG-RAG-RL-3B} & \textbf{51.8} & \textbf{49.2} & \textbf{46.7} & \textbf{49.2} \\
\textbf{BiG-RAG-RL-7B} & \textbf{56.4} & \textbf{53.1} & \textbf{50.9} & \textbf{53.5} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}

\begin{enumerate}
    \item \textbf{Bipartite graph superiority:} BiG-RAG outperforms binary KG-RAG across all datasets, validating n-ary relational representation.

    \item \textbf{Algorithmic mode effectiveness:} Zero-training BiG-RAG-Algo-GPT-4 beats GPT-4 + traditional RAG by 3.0 F1 points through structured graph retrieval.

    \item \textbf{RL mode performance:} Trained 7B model exceeds GPT-4 baseline by 14.9 F1 points despite being 20$\times$ smaller, demonstrating effectiveness of learned adaptive reasoning.

    \item \textbf{Model scaling:} Performance improves from 3B to 7B models, suggesting larger models better leverage learned retrieval strategies.
\end{enumerate}

\begin{table}[h]
\centering
\caption{Performance on Single-Hop Benchmarks}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Natural Questions} & \textbf{PopQA} & \textbf{TriviaQA} & \textbf{Avg F1} \\
\midrule
Vanilla RAG & 35.7 & 42.1 & 48.3 & 42.0 \\
GPT-4 (zero-shot) & 38.2 & 45.6 & 51.2 & 45.0 \\
\textbf{BiG-RAG-Algo-GPT-4} & \textbf{41.3} & \textbf{48.9} & \textbf{54.7} & \textbf{48.3} \\
\textbf{BiG-RAG-RL-7B} & \textbf{44.6} & \textbf{52.3} & \textbf{58.1} & \textbf{51.7} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observation:} BiG-RAG maintains advantages on simpler single-hop questions, indicating framework doesn't over-specialize to complex reasoning.

\subsection{Ablation Studies}

\begin{table}[h]
\centering
\caption{Component Ablation on 2WikiMultiHopQA}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{F1} & \textbf{$\Delta$F1} \\
\midrule
BiG-RAG-RL-7B (full) & 56.4 & --- \\
\midrule
- w/o bipartite graph (binary triples) & 48.7 & -7.7 \\
- w/o dual-path (entity only) & 52.1 & -4.3 \\
- w/o dual-path (relation only) & 51.3 & -5.1 \\
- w/o rank fusion (concat) & 54.2 & -2.2 \\
- w/o format reward & 53.8 & -2.6 \\
- w/o multi-turn (1 turn only) & 49.6 & -6.8 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{enumerate}
    \item \textbf{Bipartite graph critical:} Removing n-ary encoding causes largest performance drop (-7.7 F1), validating core architectural choice.

    \item \textbf{Dual-path synergy:} Both entity-centric and relation-centric paths contribute significantly ($\sim$4-5 F1 points each).

    \item \textbf{Rank fusion effectiveness:} Simple concatenation performs 2.2 F1 worse than reciprocal rank fusion.

    \item \textbf{Multi-turn importance:} Restricting to single retrieval turn severely degrades performance (-6.8 F1), especially on complex multi-hop questions.

    \item \textbf{Format reward utility:} Removing format reward slightly hurts performance as model generates less structured reasoning.
\end{enumerate}

\subsection{Retrieval Quality Analysis}

\begin{table}[h]
\centering
\caption{Retrieval Precision by Complexity (HotpotQA)}
\begin{tabular}{lccc}
\toprule
\textbf{Query Complexity} & \textbf{BiG-RAG} & \textbf{Binary KG} & \textbf{Vanilla RAG} \\
\midrule
Simple (1 hop) & 87.3\% & 82.1\% & 76.4\% \\
Moderate (2-3 hops) & 74.6\% & 61.2\% & 53.7\% \\
Complex ($>$3 hops) & 62.8\% & 43.5\% & 38.1\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observation:} BiG-RAG maintains higher retrieval precision across complexity levels. Gap widens for complex queries where n-ary encoding and adaptive retrieval provide greatest advantages.

\textbf{Case Study:} For query ``Which university did the director of Inception attend?'', BiG-RAG successfully performs multi-hop reasoning:
\begin{enumerate}
    \item Turn 1: Retrieves ``Inception is a 2010 science fiction film directed by Christopher Nolan''
    \item Turn 2: Reformulates sub-query ``Where did Christopher Nolan study?''
    \item Turn 3: Retrieves ``Christopher Nolan attended University College London'' and generates correct answer
\end{enumerate}

Vanilla RAG retrieves chunks about ``Inception'' but misses the connection to Nolan's education, producing incorrect answers.

\subsection{Efficiency Analysis}

\begin{table}[h]
\centering
\caption{Computational Efficiency}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Avg Latency (s)} & \textbf{Throughput (q/s)} & \textbf{Graph Build Time (hr)} \\
\midrule
Vanilla RAG & 0.8 & 125 & 2.1 \\
Binary KG-RAG & 1.4 & 71 & 8.3 \\
BiG-RAG-Algo & 1.6 & 62 & 5.4 \\
BiG-RAG-RL & 2.3 & 43 & 5.4 \\
\bottomrule
\end{tabular}
\caption*{Measured on 2WikiMultiHopQA with 10K documents, Intel Xeon Platinum 8380, NVIDIA A100.}
\end{table}

\textbf{Observations:}

\begin{enumerate}
    \item \textbf{Build time competitive:} BiG-RAG graph construction faster than binary KG despite richer representation, due to efficient extraction and indexing.

    \item \textbf{Query latency moderate:} BiG-RAG adds 0.2-0.8s overhead versus vanilla RAG but achieves substantially better accuracy. Latency acceptable for most production scenarios.

    \item \textbf{RL mode slower:} Multi-turn interaction increases latency but offline training enables using smaller models with lower inference cost long-term.
\end{enumerate}

\begin{table}[h]
\centering
\caption{Cost Analysis (per 1M queries on 2WikiMultiHopQA)}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{API Cost} & \textbf{Compute Cost} & \textbf{Total Cost} \\
\midrule
GPT-4 + RAG & \$12,000 & \$200 & \$12,200 \\
BiG-RAG-Algo-GPT-4 & \$13,500 & \$300 & \$13,800 \\
BiG-RAG-RL-7B (self-hosted) & \$0 & \$850 & \$850 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observation:} After one-time training cost ($\sim$\$500), BiG-RAG-RL dramatically reduces inference cost by avoiding API calls. Payback period $\sim$40K queries.

\subsection{Error Analysis}

\begin{table}[h]
\centering
\caption{Error Type Distribution (2WikiMultiHopQA)}
\begin{tabular}{lccc}
\toprule
\textbf{Error Type} & \textbf{BiG-RAG-RL} & \textbf{BiG-RAG-Algo} & \textbf{GPT-4 + RAG} \\
\midrule
Retrieval failure & 24\% & 31\% & 42\% \\
Reasoning error & 38\% & 41\% & 29\% \\
Format violation & 5\% & 8\% & 2\% \\
Partial answer & 18\% & 12\% & 15\% \\
Other & 15\% & 8\% & 12\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:}

\begin{enumerate}
    \item \textbf{Retrieval improvements:} BiG-RAG reduces retrieval failures compared to traditional RAG (24-31\% vs 42\%) through structured graph representation and dual-path search.

    \item \textbf{Reasoning challenges persist:} Largest error category remains reasoning errors where system retrieves correct information but generates incorrect conclusions. Future work could enhance reasoning through explicit logical verification, chain-of-thought prompting, or self-consistency.

    \item \textbf{Format violations low:} RL training effectively teaches structured reasoning format. Algorithmic mode shows slightly more violations lacking learned behavior.
\end{enumerate}

\section{Discussion}

\subsection{Advantages of Bipartite Graph Representation}

\textbf{Semantic Completeness:} N-ary encoding preserves full relational context that binary triples fragment. Experimental results show 7.7 F1 improvement over binary baseline, validating design.

\textbf{Computational Efficiency:} Despite richer representation, bipartite graphs maintain $O(\deg(v))$ query complexity. Graph construction faster than binary KG due to reduced entity resolution.

\textbf{Implementation Simplicity:} Standard graph databases directly support bipartite structures without specialized engines required for hypergraphs.

\subsection{Dual-Mode Flexibility}

\textbf{Algorithmic Mode Benefits:}
\begin{itemize}
    \item Zero training investment enables immediate deployment
    \item Deterministic behavior provides interpretability
    \item Works with any LLM (commercial or open-source)
    \item Suitable for privacy-sensitive applications
\end{itemize}

\textbf{RL Mode Benefits:}
\begin{itemize}
    \item Substantially better accuracy through learned adaptive reasoning
    \item Lower inference cost after training (no API calls)
    \item Smaller models sufficient (3-7B vs 175B for GPT-4)
    \item Customizable to domain-specific datasets
\end{itemize}

This dual-mode design uniquely balances flexibility and performance based on deployment constraints.

\subsection{Limitations and Future Work}

\textbf{Current Limitations:}

\begin{enumerate}
    \item \textbf{Entity Resolution Accuracy:} Similarity-based matching occasionally merges distinct entities with similar names. More sophisticated resolution using contextual embeddings could improve accuracy.

    \item \textbf{Extraction Quality Dependence:} System relies on LLM-based extraction which may miss relationships or hallucinate entities. Multi-stage extraction with verification or human-in-the-loop correction could help.

    \item \textbf{Static Graph Assumption:} Current implementation builds fixed graph. Real-world applications require incremental updates and temporal versioning.

    \item \textbf{Limited Multi-Modal Support:} Framework currently handles only text. Extending to images, tables, and structured data could expand applicability.
\end{enumerate}

\textbf{Future Research Directions:}

\begin{enumerate}
    \item Learned entity resolution using graph structure and contextual signals
    \item Continual learning for evolving knowledge graphs
    \item Multi-modal bipartite graphs incorporating visual and tabular knowledge
    \item Explainable retrieval with natural language path descriptions
    \item Cross-lingual extension with language-specific entity resolution
\end{enumerate}

\section{Related Systems and Comparisons}

\subsection{Comparison with Existing Graph RAG Systems}

\textbf{HippoRAG:} Uses personalized PageRank over knowledge graphs. BiG-RAG differs through n-ary relations via bipartite encoding and dual-path vector search + graph traversal versus binary relations with path-based retrieval.

\textbf{LightRAG:} Optimizes for fast build/query with binary relations. BiG-RAG prioritizes semantic completeness through n-ary encoding and provides dual operational modes.

\subsection{Comparison with RL-Based Reasoning Systems}

\textbf{ReAct:} General framework for tool use operating over APIs/search. BiG-RAG specializes for graph-structured knowledge with formal retrieval paths.

\textbf{Reflexion:} Uses self-reflection within single episode. BiG-RAG employs multi-trajectory policy optimization across episodes.

\section{Conclusion}

We presented BiG-RAG, a unified framework for knowledge-intensive question answering that addresses fundamental limitations of existing RAG systems through n-ary relational representation and adaptive multi-turn reasoning.

\textbf{Key Innovations:}

\begin{enumerate}
    \item Bipartite graph architecture preserves complete semantic context while maintaining efficient $O(\deg(v))$ query complexity

    \item Dual-path retrieval combines entity-centric and relation-centric search with reciprocal rank fusion

    \item Dual operational modes provide unprecedented flexibility: zero-training algorithmic mode for rapid deployment, optional RL mode for substantial accuracy improvements

    \item Production-grade implementation with distributed storage, async operations, and pluggable backends
\end{enumerate}

Experimental results across six benchmarks demonstrate BiG-RAG's effectiveness: algorithmic mode achieves competitive performance immediately while RL mode substantially improves accuracy---showing that 7B models trained with GRPO can match or exceed much larger systems through learned adaptive reasoning over structured knowledge graphs.

BiG-RAG establishes a practical foundation for deploying knowledge-intensive applications, bridging the gap between research prototypes and production-ready systems. The framework's dual-mode architecture enables organizations to deploy immediately using existing LLMs, then optionally enhance performance through RL training as requirements evolve.

\section*{Acknowledgments}

We thank the reviewers for their valuable feedback. This work builds upon several excellent open-source projects including Agent-R1, LightRAG, HippoRAG2, and VERL.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
